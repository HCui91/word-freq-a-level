{
 "cells": [
  {
   "source": [
    "# 从 9702 Physics 考试真题中提取高频词汇表\n",
    "\n",
    "在备课中，发现部分学生做A-Level物理考试中的计算题相对熟练，但是碰到 explain 和 define 就难以下手或答不到点子上。目前市面上也有部分国际高中物理词汇表。这篇 jupyter notebook 将统计词频和分类自动化，用量化的方式分析词汇表。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 安装所需环境\n",
    "\n",
    "1. 需要阅读PDF文件 (真题PDF请自行合并成一个文件) -> `PyPDF2`\n",
    "2. 生成 corpus 需要对 text 进行 clean -> `cleantext` 和自编的 filter\n",
    "3. 自然语言处理的 tokenise and lemmatise -> `nltk`\n",
    "4. 利用现成的自然语言模型生成单词向量 word vector -> `gensim`\n",
    "5. 利用单词向量分类单词 -> `sklearn`(`sklearn.cluster.KMeans`)\n",
    "6. 自动化翻译管线 -> `tranlators`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install requirements\n",
    "! pip install PyPDF2 clean-text nltk wordcloud gensim numpy sklearn translators\n",
    "# nltk download contents\n",
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "# to load PDF and extract corpus\n",
    "import PyPDF2\n",
    "from cleantext import clean\n",
    "from nltk import WordNetLemmatizer, tokenize, corpus\n",
    "\n",
    "# to count word frequency\n",
    "from collections import Counter\n",
    "\n",
    "# to generate word cloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# to apply pre-trained nlp model\n",
    "import gensim.downloader\n",
    "\n",
    "# to use K-Means clustering\n",
    "import numpy as np \n",
    "from sklearn.cluster import KMeans \n",
    "\n",
    "# to translate vocabulary list\n",
    "import time\n",
    "import translators as ts"
   ]
  },
  {
   "source": [
    "## 载入PDF并提取语料\n",
    "\n",
    "首先使用`PyPDF2`载入PDF文件"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PyPDF2.PdfFileReader('physics_full.pdf')"
   ]
  },
  {
   "source": [
    "将PDF转化为`plaintext`并移除下列内容:\n",
    "- 数字\n",
    "- 标点符号\n",
    "- 其他非ASCII符号\n",
    "- stopwords\n",
    "- 非英语单词\n",
    "\n",
    "最后将corpus保存在`physics_full.txt`文件中\n",
    "\n",
    "Note: 作者使用约10000页材料，此步骤大约消耗10分钟。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# open text file for saving processed corpus\n",
    "f = open('physics_full.txt','w')\n",
    "# load stop words\n",
    "stopwords = corpus.stopwords.words('english')\n",
    "# load word net lammatizer\n",
    "wn = WordNetLemmatizer()\n",
    "# load word set\n",
    "words = set(corpus.words.words())\n",
    "\n",
    "print('Start generating corpus')\n",
    "for page_num in range(reader.getNumPages()):\n",
    "\n",
    "    # report progress\n",
    "    print('Page',page_num,'/',reader.getNumPages(),end='')\n",
    "    \n",
    "    # extract plaintext from PDF\n",
    "    taster = reader.getPage(page_num).extractText()\n",
    "\n",
    "    # cleaning\n",
    "    taster = clean(taster, no_line_breaks=True, no_digits=True, replace_with_digit='', no_currency_symbols=True, replace_with_currency_symbol='')\n",
    "    \n",
    "    # tokenise and remove stopwords and non-English words\n",
    "    taster = tokenize.word_tokenize(taster)\n",
    "    taster_token = [i for i in taster if i not in stopwords and len(i) > 2 and i in words]\n",
    "    \n",
    "    # lemmatise\n",
    "    taster_lemma = [wn.lemmatize(word) for word in taster_token]\n",
    "\n",
    "    # write into corpus file\n",
    "    f.write(' '.join(taster_lemma)+' ')\n",
    "\n",
    "    # remove tmp variables to save memory\n",
    "    del taster, taster_token, taster_lemma\n",
    "    print('',end='\\r')\n",
    "\n",
    "f.close()\n",
    "\n",
    "del reader, f, stopwords, wn, words"
   ]
  },
  {
   "source": [
    "## 统计词频\n",
    "\n",
    "读取上步骤的语料，统计词频并导出词频前2000词到`counts_full.txt`文件。\n",
    "\n",
    "另生成词频前2000词的`str list`以便后续分析。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read corpus from txt file\n",
    "with open('physics_full.txt','r') as f:\n",
    "    words = f.read().split()\n",
    "    f.close()\n",
    "\n",
    "# count word frequency\n",
    "cnt = Counter(words)\n",
    "\n",
    "# generate top 2000 words\n",
    "word_list_2000 = sorted(cnt, key=cnt.get, reverse=True)[:2000]\n",
    "\n",
    "# save word frequency (counts) to a csv file\n",
    "with open('counts_full.csv','w') as f:\n",
    "    for k, v in cnt.most_common():\n",
    "        f.write(k + ',' + str(v) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "# save top 2000 words as a one word per line txt\n",
    "with open('top_2000_word.txt','w') as f:\n",
    "    for x in word_list_2000:\n",
    "        f.writelines(x + '\\n')"
   ]
  },
  {
   "source": [
    "### 生成词云\n",
    "\n",
    "大家喜闻乐见的词云 (宣传用)\n",
    "\n",
    "挑选前200词并输出为2000x1000的透明底PNG图片。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate word could clss\n",
    "wordcloud = WordCloud(width=2000,height=1000,max_font_size=400,background_color='rgba(255, 255, 255, 0)', mode='RGBA').generate(' '.join(words))\n",
    "\n",
    "# save image\n",
    "wordcloud.to_file('physics_full_wordcloud.png')"
   ]
  },
  {
   "source": [
    "## 使用预训练模型生成单词向量\n",
    "\n",
    "这里使用`fasttext-wiki-news-subwords-300`模型。根据 https://github.com/RaRe-Technologies/gensim-data ，此模型基于160亿token训练单词向量，并包含了Wikipedia 2017的数据。Wikipedia大概率会有相关物理单词的词条，因此使用此模型生成单词向量。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word vector model\n",
    "glove_vectors = gensim.downloader.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "print(glove_vectors)\n",
    "print(glove_vectors['physics'])"
   ]
  },
  {
   "source": [
    "生成一个`numpy`矩阵来保存词频前2000的单词向量。\n",
    "\n",
    "Note: 也有可能此模型不包含个别单词的情况，目前只能先跳过。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word list\n",
    "with open('top_2000_word.txt','r') as f:\n",
    "    word_list_2000 = []\n",
    "    for line in f:\n",
    "        word_list_2000.append(line.strip())\n",
    "\n",
    "# empty array for word vectors\n",
    "x = np.zeros((len(word_list_2000),len(glove_vectors['physics'])))\n",
    "\n",
    "# assign word vectors to the array\n",
    "for index, word in enumerate(word_list_2000):\n",
    "    try:\n",
    "        x[index,:] = glove_vectors[word]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "source": [
    "## 使用单词向量聚类\n",
    "\n",
    "利用预训练的K-Means算法将上步骤生成的单词向量分类成20类。此20类是考虑每个分类不高于100词为目标，方便学生背诵。\n",
    "\n",
    "导出分组信息到`top_2000_word_group.txt`以便后续翻译。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of clusters\n",
    "clusters = 24\n",
    "\n",
    "# apply KMeans to the data\n",
    "kmeans = KMeans(n_clusters=clusters)\n",
    "kmeans.fit_predict(x)\n",
    "\n",
    "# save grouping data to file\n",
    "np.savetxt('top_2000_word_group.txt',kmeans.labels_)"
   ]
  },
  {
   "source": [
    "### 简易的 K-Means 性能分析\n",
    "\n",
    "使用 Sum of Squred Error (SSE) 来统计单词向量到中心点的距离之和。SSE 越大说明 cluster 越分散。SSE 和 cluster 数量制图，折线的异常低点是我们希望找到的最适合的 cluster 数量。如果此折线未见到明显的转折点，说明 word vector 未显示明显分类。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse performance of k-means clustering\n",
    "score = {}\n",
    "for clusters in range(1,30):\n",
    "    kmeans = KMeans(n_clusters=clusters).fit(x)\n",
    "    score[clusters] = kmeans.inertia_\n",
    "\n",
    "plt.figure(0,figsize=(6,4))    \n",
    "plt.scatter(list(score.keys()),list(score.values()))\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('SSE')"
   ]
  },
  {
   "source": [
    "## 自动化翻译\n",
    "\n",
    "这里简单介绍两种自动化翻译的方法。\n",
    "\n",
    "1. 利用翻译API自动翻译单词表\n",
    "2. Google translate 文档翻译\n",
    "\n",
    "### 利用翻译API自动翻译单词表\n",
    "\n",
    "2000个单词翻译起来也不是轻松的事情。最基础的自动化便是使用开放的翻译API来翻译单词。当然，翻译效果比较差也不符合语境。但是可以减少人工翻译的时间。\n",
    "\n",
    "最后将`单词, 组别, 自动翻译`输出到`physics_full_group.csv`以便后续调整"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word list\n",
    "with open('top_2000_word.dat','rb') as f:\n",
    "    word_list_2000 = pickle.load(f)\n",
    "\n",
    "# load grouping data\n",
    "top_2000_word_group = np.loadtxt('top_2000_word_group.txt',delimiter=',')\n",
    "\n",
    "# open final translated output\n",
    "groupout = open('physics_full_group.csv','w')\n",
    "\n",
    "# reset counter\n",
    "counter = 0\n",
    "\n",
    "print('Translate:')\n",
    "for word,group in zip(word_list_2000,top_2000_word_group):\n",
    "\n",
    "    # report translate progress\n",
    "    print('Working on word',counter+1, '/',len(word_list_2000),end='')\n",
    "    \n",
    "    # request translate\n",
    "    translation = ts.google(word,to_language='zh')\n",
    "\n",
    "    # write into the output\n",
    "    groupout.writelines(str(word) + ',' + str(int(group))+ ',' + str(translation) +'\\n')\n",
    "\n",
    "    counter+=1\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "    print('',end='\\r')\n",
    "    \n",
    "groupout.close()"
   ]
  },
  {
   "source": [
    "## Google translate 文档翻译\n",
    "\n",
    "上一个方法对于本项目有一丝困难，因为各类翻译 API 在 request 200个单词的时候差不多就不再响应了。的确每次翻译单个单词会让 API 拒绝接受请求来降低流量。这里另辟蹊径，使用文档翻译来翻译单词表。\n",
    "\n",
    "导出为 one word per line 的 txt 文件 `top_2000_word.txt`，然后上传到 https://translate.google.cn/?sl=en&tl=zh-CN&op=docs 后进行文档翻译。\n",
    "\n",
    "翻译结果网页另存为 `top_2000_word_translated.txt` 后程序读取到 `word_list_2000_translated` 列表。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load translated txt file\n",
    "with open('top_2000_word_translated.txt','r') as f:\n",
    "    word_list_2000_translated = []\n",
    "    for line in f:\n",
    "        word_list_2000_translated.append(line.strip())"
   ]
  },
  {
   "source": [
    "生成和上方法同样的 `单词, 组别, 自动翻译` 格式的 `physics_full_group.csv` 文件"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open final translated output\n",
    "groupout = open('physics_full_group.csv','w')\n",
    "\n",
    "# load grouping data\n",
    "top_2000_word_group = np.loadtxt('top_2000_word_group.txt',delimiter=',')\n",
    "\n",
    "for word,group,translation in zip(word_list_2000,top_2000_word_group,word_list_2000_translated):\n",
    "    # write into the output\n",
    "    groupout.writelines(str(word) + ',' + str(int(group))+ ',' + str(translation) +'\\n')\n",
    "\n",
    "groupout.close()"
   ]
  },
  {
   "source": [
    "## 总结\n",
    "\n",
    "此套自动统计词频并分组脚本可以比较稳定的输出给定文件的高频词汇。利用预训练模型可以对高频词汇进行词汇分类 (word embedding)。对于大量的单词，可以调用翻译 API 或输出文档进行文档翻译，减轻工作量。\n",
    "\n",
    "### 目前的不足和计划\n",
    "\n",
    "- PDF处理\n",
    "\n",
    "    - PDF可以不用提前进行合并，而是使用 `glob` module 来对目录进行搜索。\n",
    "\n",
    "- NLP相关\n",
    "\n",
    "    - 对 text 进行 cleaning 时移除 punctuations，使得句子信息丢失。如果不移除，语料可保留以句为单位的信息。对于未来的句子语义理解会有帮助。\n",
    "\n",
    "    - Word embedding 的 performance 没有进行分析。\n",
    "\n",
    "- ML相关\n",
    "\n",
    "    - K-Means clustering 的 performance 没有分析。clustering 真的收敛了吗？\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}